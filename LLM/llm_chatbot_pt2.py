# -*- coding: utf-8 -*-
"""LLM Chatbot PT2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XDEayKR4IJfa6EYijRGwAMCekFvyrUbr
"""

!pip install streamlit transformers torch

import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# 1. Model selection and loading
MODEL_NAME = "google/flan-t5-small"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

@st.cache_resource(show_spinner=False)
def load_model_and_tokenizer(name):
    tokenizer = AutoTokenizer.from_pretrained(name)
    model = AutoModelForSeq2SeqLM.from_pretrained(name).to(device)
    return tokenizer, model

tokenizer, model = load_model_and_tokenizer(MODEL_NAME)

# 2. Local text generation function
def generate_response(prompt_text: str) -> str:
    prompt = f"Answer the following question: {prompt_text}"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    output_tokens = model.generate(**inputs, max_new_tokens=64)
    return tokenizer.decode(output_tokens[0], skip_special_tokens=True)

# 3. Streamlit user interface setup
st.set_page_config(page_title="ðŸ’¬ Local Chatbot", page_icon="ðŸ’¡")
st.title("ðŸ’¡ LLM Chatbot (Local - Flan-T5-Small)")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

user_message = st.chat_input("Type your question here...")

if user_message:
    # Save user input
    st.session_state.chat_history.append(("user", user_message))
    # Generate model response
    with st.spinner("Thinking ðŸ’­..."):
        response = generate_response(user_message)
    st.session_state.chat_history.append(("bot", response))

# Display conversation history
for role, text in st.session_state.chat_history:
    if role == "user":
        st.chat_message("user").write(text)
    else:
        st.chat_message("assistant").write(text)